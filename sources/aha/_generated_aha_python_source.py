# ==============================================================================
# Merged Lakeflow Source: aha
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import (
    Any,
    Dict,
    Iterator,
    List,
    Optional,
    Tuple,
)
import json
import re
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from urllib.parse import quote_plus
from pyspark.sql.types import *
import base64
import logging
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/aha/aha.py
    ########################################################

    logger = logging.getLogger(__name__)


    class LakeflowConnect:
        """
        Aha! connector for Lakeflow Community Connectors.

        Ingests ideas, proxy votes, and comments from the Aha! API.
        Uses CDC (Change Data Capture) synchronization with updated_since filtering.
        """

        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Aha! connector.

            Args:
                options: Dictionary containing:
                    - api_key: Aha! API key (Bearer token)
                    - subdomain: Aha! subdomain (e.g., "company" for company.aha.io)
            """
            self.api_key = options["api_key"]
            self.subdomain = options["subdomain"]
            # Optional global filters/safeguards
            self.ideas_workflow_status: Optional[str] = options.get("ideas_workflow_status")
            self.max_ideas: Optional[int] = self._parse_optional_int(options.get("max_ideas"))
            self.base_url = f"https://{self.subdomain}.aha.io/api/v1"

            self._session = requests.Session()
            self._session.headers.update(
                {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                    "Accept": "application/json",
                }
            )
            # Cache for ideas list, keyed by (workflow_status, q, max_ideas, updated_since).
            # Keep None as the "cleared" state for compatibility with existing tests.
            self._ideas_cache: Optional[dict[Tuple[Optional[str], Optional[str], Optional[int], Optional[str]], list[dict]]] = None

        def list_tables(self) -> List[str]:
            """Return the list of available Aha! tables."""
            return ["ideas", "idea_proxy_votes", "idea_comments"]

        def get_table_schema(
            self, table_name: str, table_options: Dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.
            """
            schemas = {
                "ideas": StructType(
                    [
                        StructField("id", StringType()),
                        StructField("reference_num", StringType()),
                        StructField("name", StringType()),
                        StructField("description", StringType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("votes", LongType()),
                        StructField("endorsements_count", LongType()),
                        StructField("comments_count", LongType()),
                        StructField("url", StringType()),
                        StructField(
                            "workflow_status",
                            StructType(
                                [
                                    StructField("id", StringType()),
                                    StructField("name", StringType()),
                                    StructField("complete", BooleanType()),
                                ]
                            ),
                        ),
                    ]
                ),
                "idea_proxy_votes": StructType(
                    [
                        StructField("id", StringType()),
                        StructField("idea_id", StringType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("value", LongType()),
                        StructField("link", StringType()),
                        StructField("weight", LongType()),
                        StructField(
                            "endorsed_by_portal_user",
                            StructType(
                                [
                                    StructField("id", StringType()),
                                    StructField("name", StringType()),
                                    StructField("email", StringType()),
                                    StructField("created_at", StringType()),
                                ]
                            ),
                        ),
                        StructField(
                            "endorsed_by_idea_user",
                            StructType(
                                [
                                    StructField("id", StringType()),
                                    StructField("name", StringType()),
                                    StructField("email", StringType()),
                                    StructField("created_at", StringType()),
                                    StructField("title", StringType()),
                                ]
                            ),
                        ),
                        StructField(
                            "idea_organization",
                            StructType(
                                [
                                    StructField("id", StringType()),
                                    StructField("name", StringType()),
                                    StructField("created_at", StringType()),
                                    StructField("url", StringType()),
                                    StructField("resource", StringType()),
                                ]
                            ),
                        ),
                        StructField(
                            "idea_users",
                            ArrayType(
                                StructType(
                                    [
                                        StructField("id", StringType()),
                                        StructField("first_name", StringType()),
                                        StructField("last_name", StringType()),
                                        StructField("email", StringType()),
                                    ]
                                )
                            ),
                        ),
                        StructField(
                            "description",
                            StructType(
                                [
                                    StructField("id", StringType()),
                                    StructField("body", StringType()),
                                    StructField("editor_version", LongType()),
                                    StructField("created_at", StringType()),
                                    StructField("updated_at", StringType()),
                                    StructField(
                                        "attachments",
                                        ArrayType(MapType(StringType(), StringType())),
                                    ),
                                ]
                            ),
                        ),
                        StructField(
                            "custom_fields",
                            ArrayType(MapType(StringType(), StringType())),
                        ),
                    ]
                ),
                "idea_comments": StructType(
                    [
                        StructField("id", StringType()),
                        StructField("idea_id", StringType()),
                        StructField("body", StringType()),
                        StructField("created_at", StringType()),
                        StructField("updated_at", StringType()),
                        StructField("url", StringType()),
                        StructField("resource", StringType()),
                        StructField(
                            "user",
                            StructType(
                                [
                                    StructField("id", StringType()),
                                    StructField("name", StringType()),
                                    StructField("email", StringType()),
                                    StructField("created_at", StringType()),
                                    StructField("updated_at", StringType()),
                                ]
                            ),
                        ),
                        StructField(
                            "attachments",
                            ArrayType(
                                StructType(
                                    [
                                        StructField("id", StringType()),
                                        StructField("download_url", StringType()),
                                        StructField("created_at", StringType()),
                                        StructField("file_name", StringType()),
                                        StructField("file_size", LongType()),
                                        StructField("content_type", StringType()),
                                    ]
                                )
                            ),
                        ),
                    ]
                ),
            }

            if table_name not in schemas:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return schemas[table_name]

        def read_table_metadata(
            self, table_name: str, table_options: Dict[str, str]
        ) -> dict:
            """
            Fetch the metadata of a table.
            All tables use CDC ingestion type with updated_at as cursor field.
            """
            metadata = {
                "ideas": {
                    "primary_keys": ["id"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "cdc",
                },
                "idea_proxy_votes": {
                    "primary_keys": ["id"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "cdc",
                },
                "idea_comments": {
                    "primary_keys": ["id"],
                    "cursor_field": "updated_at",
                    "ingestion_type": "cdc",
                },
            }

            if table_name not in metadata:
                raise ValueError(f"Table '{table_name}' is not supported.")

            return metadata[table_name]

        def read_table(
            self, table_name: str, start_offset: dict, table_options: Dict[str, str]
        ) -> (Iterator[dict], dict):
            """Read data from the specified Aha! table with CDC support."""
            if table_name == "ideas":
                return self._read_ideas(start_offset, table_options)
            elif table_name == "idea_proxy_votes":
                return self._read_proxy_votes(start_offset, table_options)
            elif table_name == "idea_comments":
                return self._read_comments(start_offset, table_options)
            else:
                raise ValueError(f"Table '{table_name}' is not supported.")

        @staticmethod
        def _parse_optional_int(value: Optional[str]) -> Optional[int]:
            if value is None:
                return None
            if isinstance(value, int):
                return value
            s = str(value).strip()
            if not s:
                return None
            return int(s)

        def _effective_ideas_workflow_status(self, table_options: Dict[str, str]) -> Optional[str]:
            # Per-table override, else connector default.
            return table_options.get("ideas_workflow_status") or self.ideas_workflow_status

        def _effective_max_ideas(self, table_options: Dict[str, str]) -> Optional[int]:
            override = self._parse_optional_int(table_options.get("max_ideas"))
            return override if override is not None else self.max_ideas

        def _effective_ideas_q(self, table_options: Dict[str, str]) -> Optional[str]:
            # Support q as a convenience alias for ideas table.
            return table_options.get("ideas_q") or table_options.get("q")

        def _get_ideas(self, start_offset: dict, table_options: Dict[str, str]) -> list[dict]:
            """
            Get ideas, using cache if available.
            Supports CDC filtering via updated_since from start_offset.
            This prevents duplicate API calls when reading child tables.
            """
            workflow_status = self._effective_ideas_workflow_status(table_options)
            q = self._effective_ideas_q(table_options)
            max_ideas = self._effective_max_ideas(table_options)
            updated_since = start_offset.get("updated_since") if start_offset else None
            cache_key = (workflow_status, q, max_ideas, updated_since)

            if self._ideas_cache is None:
                self._ideas_cache = {}

            if cache_key not in self._ideas_cache:
                extra_params: dict[str, str] = {}
                if workflow_status:
                    extra_params["workflow_status"] = workflow_status
                if q:
                    # Aha docs: q searches against idea name; pass-through.
                    extra_params["q"] = q
                if updated_since:
                    # CDC: only fetch ideas updated since the last checkpoint
                    extra_params["updated_since"] = updated_since

                # Fetch at most max_ideas ideas (early stop) to bound work.
                self._ideas_cache[cache_key] = self._fetch_paginated(
                    "/ideas",
                    "ideas",
                    extra_params=extra_params if extra_params else None,
                    max_records=max_ideas,
                )

            return self._ideas_cache[cache_key]

        def clear_cache(self) -> None:
            """Clear the ideas cache. Useful between test runs."""
            self._ideas_cache = None

        @staticmethod
        def _find_max_updated_at(records: list[dict], fallback: Optional[str] = None) -> Optional[str]:
            """
            Find the maximum updated_at value from records.
            Used to compute the next offset for CDC.
            """
            max_val = fallback
            for record in records:
                updated_at = record.get("updated_at")
                if updated_at and (max_val is None or updated_at > max_val):
                    max_val = updated_at
            return max_val

        def _fetch_paginated(
            self,
            endpoint: str,
            response_key: str,
            extra_params: dict | None = None,
            max_records: Optional[int] = None,
        ) -> list[dict]:
            """
            Generic pagination handler for Aha! API.

            Args:
                endpoint: API endpoint (e.g., "/ideas")
                response_key: Key in response JSON containing the records
                extra_params: Additional query parameters

            Returns:
                List of all records across all pages
            """
            records = []
            page = 1
            per_page = 100

            while True:
                # Build URL - handle endpoints that already have query params
                if "?" in endpoint:
                    url = f"{self.base_url}{endpoint}&page={page}&per_page={per_page}"
                else:
                    url = f"{self.base_url}{endpoint}?page={page}&per_page={per_page}"

                # Add extra params if provided
                if extra_params:
                    for key, value in extra_params.items():
                        url += f"&{key}={quote_plus(str(value))}"

                resp = None
                max_attempts = 3
                for attempt in range(1, max_attempts + 1):
                    resp = self._session.get(url)

                    # Handle vendor throttling with a bounded retry/backoff.
                    if resp.status_code == 429 and attempt < max_attempts:
                        sleep_s = self._throttle_sleep_seconds(resp)
                        logger.warning(
                            "Aha API throttled (429) for %s; sleeping %ss then retrying (attempt %d/%d)",
                            endpoint,
                            sleep_s,
                            attempt,
                            max_attempts,
                        )
                        time.sleep(sleep_s)
                        continue

                    break

                assert resp is not None
                if resp.status_code != 200:
                    logger.error(f"API error for {endpoint}: {resp.status_code} {resp.text}")
                    raise Exception(
                        f"Aha! API error for {endpoint}: {resp.status_code} {resp.text}"
                    )

                data = resp.json()
                page_records = data.get(response_key, [])
                records.extend(page_records)

                # Early stop if we reached max_records
                if max_records is not None and len(records) >= max_records:
                    break

                # Check pagination
                pagination = data.get("pagination", {})
                total_pages = pagination.get("total_pages", 1)

                if page >= total_pages:
                    break

                page += 1

            return records

        @staticmethod
        def _throttle_sleep_seconds(resp) -> int:
            """
            Choose a conservative sleep duration for 429s.
            - Prefer Retry-After header when present.
            - Otherwise parse Aha's message like 'Try again in 60 seconds.'
            - Fallback to 60.
            """
            retry_after = resp.headers.get("Retry-After") if hasattr(resp, "headers") else None
            if retry_after:
                try:
                    return max(1, int(float(retry_after)))
                except Exception:
                    pass

            try:
                data = resp.json()
                errors = data.get("errors") if isinstance(data, dict) else None
                if isinstance(errors, list) and errors:
                    msg = errors[0].get("message")
                    if isinstance(msg, str):
                        m = re.search(r"(\d+)\s*seconds", msg)
                        if m:
                            return max(1, int(m.group(1)))
            except Exception:
                pass

            return 60

        def _read_ideas(self, start_offset: dict, table_options: Dict[str, str]) -> (Iterator[dict], dict):
            """Read ideas from Aha! with CDC support."""
            updated_since = start_offset.get("updated_since") if start_offset else None
            logger.info(f"Fetching ideas from Aha! (updated_since={updated_since})")
            ideas = self._get_ideas(start_offset, table_options)
            logger.info(f"Fetched {len(ideas)} ideas")
            # Flatten description.body -> description
            for idea in ideas:
                desc = idea.get("description")
                if isinstance(desc, dict):
                    idea["description"] = desc.get("body")
                else:
                    idea["description"] = None
                    if desc is not None:
                        logger.warning(
                            f"Could not extract description for idea {idea.get('id')}: "
                            f"expected dict, got {type(desc).__name__}"
                        )
            # Compute next offset from max updated_at
            max_updated_at = self._find_max_updated_at(ideas, updated_since)
            next_offset = {"updated_since": max_updated_at} if max_updated_at else (start_offset or {})
            return iter(ideas), next_offset

        def _read_proxy_votes(self, start_offset: dict, table_options: Dict[str, str]) -> (Iterator[dict], dict):
            """
            Read proxy votes for ideas with CDC support.
            Uses cached ideas list to avoid duplicate API calls.
            """
            updated_since = start_offset.get("updated_since") if start_offset else None
            records = []
            ideas = self._get_ideas(start_offset, table_options)
            total_ideas = len(ideas)

            logger.info(f"Fetching proxy votes for {total_ideas} ideas (updated_since={updated_since})")

            for i, idea in enumerate(ideas):
                idea_id = idea.get("id")
                if not idea_id:
                    logger.warning(f"Idea at index {i} has no id, skipping")
                    continue

                # Log progress every 50 ideas
                if (i + 1) % 50 == 0 or i == 0:
                    logger.info(f"Processing idea {i + 1}/{total_ideas} for proxy votes")

                # Fetch proxy votes for this idea
                votes = self._fetch_paginated(
                    f"/ideas/{idea_id}/endorsements?proxy=true", "idea_endorsements"
                )

                for vote in votes:
                    # Ensure idea_id is present in each vote record
                    vote["idea_id"] = idea_id
                    records.append(vote)

            logger.info(f"Fetched {len(records)} proxy votes across {total_ideas} ideas")
            # Compute next offset from max updated_at
            max_updated_at = self._find_max_updated_at(records, updated_since)
            next_offset = {"updated_since": max_updated_at} if max_updated_at else (start_offset or {})
            return iter(records), next_offset

        def _read_comments(self, start_offset: dict, table_options: Dict[str, str]) -> (Iterator[dict], dict):
            """
            Read comments for ideas with CDC support.
            Uses cached ideas list to avoid duplicate API calls.
            """
            updated_since = start_offset.get("updated_since") if start_offset else None
            records = []
            ideas = self._get_ideas(start_offset, table_options)
            total_ideas = len(ideas)

            logger.info(f"Fetching comments for {total_ideas} ideas (updated_since={updated_since})")

            for i, idea in enumerate(ideas):
                idea_id = idea.get("id")
                if not idea_id:
                    logger.warning(f"Idea at index {i} has no id, skipping")
                    continue

                # Log progress every 50 ideas
                if (i + 1) % 50 == 0 or i == 0:
                    logger.info(f"Processing idea {i + 1}/{total_ideas} for comments")

                # Fetch comments for this idea
                comments = self._fetch_paginated(
                    f"/ideas/{idea_id}/comments", "comments"
                )

                for comment in comments:
                    # Ensure idea_id is present in each comment record
                    comment["idea_id"] = idea_id
                    records.append(comment)

            logger.info(f"Fetched {len(records)} comments across {total_ideas} ideas")
            # Compute next offset from max updated_at
            max_updated_at = self._find_max_updated_at(records, updated_since)
            next_offset = {"updated_since": max_updated_at} if max_updated_at else (start_offset or {})
            return iter(records), next_offset


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
